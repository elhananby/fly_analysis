{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Set this before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.signal import savgol_filter\n",
    "import fly_analysis as fa\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import scipy\n",
    "import pynumdiff\n",
    "\n",
    "date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(date_time)\n",
    "\n",
    "os.makedirs(f\"Figures/dnp03_paper/{date_time}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaccadeConfig:\n",
    "    \"\"\"Configuration parameters for saccade detection and analysis\"\"\"\n",
    "\n",
    "    n_before: int = 100  # frames before saccade\n",
    "    n_after: int = 100  # frames after saccade\n",
    "    stim_window: int = 50  # window to look for stimulus-elicited saccades\n",
    "    min_group_length: int = 200\n",
    "    delay: int = 15  # delay for no-saccade cases\n",
    "    ang_vel_threshold: float = np.deg2rad(300)  # mianimum angular velocity for saccade\n",
    "    lin_vel_threshold: float = 0.01  # minimum linear velocity for saccade\n",
    "    peak_distance: int = 10  # minimum distance between peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_smooth(df):\n",
    "    \"\"\"\n",
    "    Applies Savitzky-Golay filter to smooth specified columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the columns to be smoothed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with smoothed columns.\n",
    "\n",
    "    The function applies the Savitzky-Golay filter with a window length of 21 and a polynomial order of 3\n",
    "    to the columns 'x', 'y', 'z', 'xvel', 'yvel', and 'zvel' in the input DataFrame.\n",
    "    \"\"\"\n",
    "    columns = [\"x\", \"y\", \"z\", \"xvel\", \"yvel\", \"zvel\"]\n",
    "    df[columns] = df[columns].apply(lambda x: savgol_filter(x, 21, 3))\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_mean_and_std(data, ax, label=None):\n",
    "    data = np.abs(data)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "\n",
    "    ax.plot(mean, label=label)\n",
    "    ax.fill_between(np.arange(len(mean)), mean - std, mean + std, alpha=0.2)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def polar_histogram(\n",
    "    ax,\n",
    "    angles,\n",
    "    bins=16,\n",
    "    density=None,\n",
    "    offset=0,\n",
    "    lab_unit=\"degrees\",\n",
    "    start_zero=False,\n",
    "    **param_dict,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot polar histogram of angles on ax. ax must have been created using\n",
    "    subplot_kw=dict(projection='polar'). Angles are expected in radians.\n",
    "    \"\"\"\n",
    "    # Wrap angles to [-pi, pi)\n",
    "    angles = (angles + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "    # Set bins symetrically around zero\n",
    "    if start_zero:\n",
    "        # To have a bin edge at zero use an even number of bins\n",
    "        if bins % 2:\n",
    "            bins += 1\n",
    "        bins = np.linspace(-np.pi, np.pi, num=bins + 1)\n",
    "\n",
    "    # Bin data and record counts\n",
    "    count, bin = np.histogram(angles, bins=bins)\n",
    "\n",
    "    # Compute width of each bin\n",
    "    widths = np.diff(bin)\n",
    "\n",
    "    # By default plot density (frequency potentially misleading)\n",
    "    if density is None or density is True:\n",
    "        # Area to assign each bin\n",
    "        area = count / angles.size\n",
    "        # Calculate corresponding bin radius\n",
    "        radius = (area / np.pi) ** 0.5\n",
    "    else:\n",
    "        radius = count\n",
    "\n",
    "    # Plot data on ax\n",
    "    ax.bar(\n",
    "        bin[:-1],\n",
    "        radius,\n",
    "        zorder=1,\n",
    "        align=\"edge\",\n",
    "        width=widths,\n",
    "        edgecolor=\"C0\",\n",
    "        fill=False,\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    # Set the direction of the zero angle\n",
    "    ax.set_theta_offset(offset)\n",
    "\n",
    "    # Remove ylabels, they are mostly obstructive and not informative\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    if lab_unit == \"radians\":\n",
    "        label = [\n",
    "            \"$0$\",\n",
    "            r\"$\\pi/4$\",\n",
    "            r\"$\\pi/2$\",\n",
    "            r\"$3\\pi/4$\",\n",
    "            r\"$\\pi$\",\n",
    "            r\"$5\\pi/4$\",\n",
    "            r\"$3\\pi/2$\",\n",
    "            r\"$7\\pi/4$\",\n",
    "        ]\n",
    "        ax.set_xticklabels(label)\n",
    "\n",
    "\n",
    "def plot_histogram(arr, ax=None, label=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the given array with a density estimate.\n",
    "\n",
    "    Parameters:\n",
    "    arr (array-like): The input data array to plot.\n",
    "    ax (matplotlib.axes.Axes, optional): The axes on which to plot the histogram.\n",
    "                                         If None, a new figure and axes are created.\n",
    "    label (str, optional): The label for the histogram.\n",
    "\n",
    "    Returns:\n",
    "    matplotlib.axes.Axes: The axes with the plotted histogram.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    sns.histplot(\n",
    "        arr,\n",
    "        ax=ax,\n",
    "        bins=36,\n",
    "        binrange=(-np.pi, np.pi),\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def calculate_inverted_signed_angle(v1, v2):\n",
    "    # Calculate the signed angle between two vectors\n",
    "    angle = np.arctan2(np.cross(v1, v2), np.dot(v1, v2))\n",
    "\n",
    "    # Invert the angle\n",
    "    if angle >= 0:\n",
    "        inverted_angle = np.pi - angle\n",
    "    else:\n",
    "        inverted_angle = -np.pi - angle\n",
    "\n",
    "    return inverted_angle  # This will be in radians, in the range [-π, π]\n",
    "\n",
    "\n",
    "def get_heading_difference_for_trajectory(xyz, midpoint=65, n_around=10):\n",
    "    if np.shape(xyz)[1] == 3:\n",
    "        xyz = xyz[:, :2]\n",
    "\n",
    "    vector_before = xyz[midpoint - n_around] - xyz[midpoint]\n",
    "    vector_after = xyz[midpoint + n_around] - xyz[midpoint]\n",
    "\n",
    "    return calculate_inverted_signed_angle(vector_before, vector_after)\n",
    "\n",
    "\n",
    "def get_all_saccade_data(df, stim, save_plots=False, plot_dir=None, **kwargs):\n",
    "    config = SaccadeConfig(**kwargs)\n",
    "\n",
    "    # initialize results dictionary\n",
    "    results = {\n",
    "        \"angular_velocity\": [],\n",
    "        \"linear_velocity\": [],\n",
    "        \"heading_diff\": [],\n",
    "        \"group\": [],\n",
    "    }\n",
    "\n",
    "    # iterate over each stimulus\n",
    "    for _, row in tqdm(stim.iterrows(), total=len(stim)):\n",
    "        # get group data\n",
    "        group_mask = (df[\"obj_id\"] == row[\"obj_id\"]) & (df[\"exp_num\"] == row[\"exp_num\"])\n",
    "        grp = df[group_mask].copy()\n",
    "\n",
    "        # skip groups with insufficient data\n",
    "        if len(grp) < config.min_group_length:\n",
    "            continue\n",
    "\n",
    "        # get stimulus index\n",
    "        stim_idx = np.where(grp[\"frame\"] == row[\"frame\"])[0]\n",
    "        if len(stim_idx) == 0:  # skip if stimulus not found\n",
    "            stim_idx = None\n",
    "        else:  # take first index if multiple found\n",
    "            stim_idx = stim_idx[0]\n",
    "\n",
    "        # Smooth data\n",
    "        grp = sg_smooth(grp)\n",
    "\n",
    "        # Calculate angular and linear velocity\n",
    "        xvel, yvel = grp[\"xvel\"].to_numpy(), grp[\"yvel\"].to_numpy()\n",
    "        x, y, z = grp[\"x\"].to_numpy(), grp[\"y\"].to_numpy(), grp[\"z\"].to_numpy()\n",
    "\n",
    "        # Calculate angular and linear velocity\n",
    "        theta = np.arctan2(yvel, xvel)\n",
    "        theta_unwrap = np.unwrap(theta)\n",
    "        _, angular_velocity = pynumdiff.smooth_finite_difference.butterdiff(\n",
    "            theta_unwrap, dt=0.01, params=[1, 0.1]\n",
    "        )\n",
    "        linear_velocity = np.sqrt(xvel**2 + yvel**2)\n",
    "\n",
    "        saccades, _ = scipy.signal.find_peaks(\n",
    "            np.abs(angular_velocity),\n",
    "            height=config.ang_vel_threshold,\n",
    "            distance=config.peak_distance,\n",
    "        )\n",
    "\n",
    "        for sac_idx in saccades:\n",
    "            # skip saccades at the beginning and end of the experiment\n",
    "            if sac_idx - config.n_before < 0 or sac_idx + config.n_after >= len(\n",
    "                angular_velocity\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # skip saccades around stimulus\n",
    "            if stim_idx is not None:\n",
    "                if stim_idx <= sac_idx <= stim_idx + config.stim_window:\n",
    "                    continue\n",
    "\n",
    "            # check if saccade is in center\n",
    "            if (\n",
    "                (0.05 <= z[sac_idx] <= 0.25)\n",
    "                and (-0.2 <= x[sac_idx] <= 0.2)\n",
    "                and (-0.2 <= y[sac_idx] <= 0.2)\n",
    "            ):\n",
    "                results[\"angular_velocity\"].append(\n",
    "                    angular_velocity[\n",
    "                        sac_idx - config.n_before : sac_idx + config.n_after\n",
    "                    ]\n",
    "                )\n",
    "                results[\"linear_velocity\"].append(\n",
    "                    linear_velocity[\n",
    "                        sac_idx - config.n_before : sac_idx + config.n_after\n",
    "                    ]\n",
    "                )\n",
    "                results[\"heading_diff\"].append(\n",
    "                    get_heading_difference_for_trajectory(\n",
    "                        grp[[\"x\", \"y\"]].values, sac_idx\n",
    "                    )\n",
    "                )\n",
    "                results[\"group\"].append(\"spontaneous\")\n",
    "\n",
    "        # get stimulus-elicited saccades\n",
    "        if stim_idx is not None:\n",
    "            response_idx = stim_idx + config.delay\n",
    "            if (\n",
    "                response_idx - config.n_before < 0\n",
    "                or response_idx + config.n_after >= len(angular_velocity)\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            angvel = angular_velocity[\n",
    "                response_idx - config.n_before : response_idx + config.n_after\n",
    "            ]\n",
    "            linvel = linear_velocity[\n",
    "                response_idx - config.n_before : response_idx + config.n_after\n",
    "            ]\n",
    "            hd = get_heading_difference_for_trajectory(\n",
    "                grp[[\"x\", \"y\"]].values, response_idx\n",
    "            )\n",
    "\n",
    "            results[\"angular_velocity\"].append(angvel)\n",
    "            results[\"linear_velocity\"].append(linvel)\n",
    "            results[\"heading_diff\"].append(hd)\n",
    "            results[\"group\"].append(\"stimulus\")\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    for key in results:\n",
    "        results[key] = np.array(results[key])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def group_index(results, group):\n",
    "    return np.array([i for i, x in enumerate(results[\"group\"]) if x == group]).astype(\n",
    "        int\n",
    "    )\n",
    "\n",
    "\n",
    "def opto_summary_plot(data: dict, group: str):\n",
    "    fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(15, 5))\n",
    "\n",
    "    for response in np.unique(data[\"group\"]):\n",
    "        idx = group_index(data, response)\n",
    "        angvel = np.rad2deg(data[\"angular_velocity\"][idx])\n",
    "        linvel = data[\"linear_velocity\"][idx]\n",
    "        heading_diff = data[\"heading_diff\"][idx]\n",
    "\n",
    "        plot_mean_and_std(angvel, axs[0], label=f\"{response} n={len(angvel)}\")\n",
    "        plot_mean_and_std(linvel, axs[1])\n",
    "        plot_histogram(heading_diff, axs[2])\n",
    "\n",
    "    fig.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"/home/buchsbaum/mnt/md0/Experiments/\"\n",
    "checkpoint_path = \"/home/buchsbaum/src/fly_analysis/notebooks/checkpoints/\"\n",
    "data_path = \"/home/buchsbaum/src/fly_analysis/notebooks/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Literal\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "StorageFormat = Literal[\"parquet\", \"hdf5\", \"feather\"]\n",
    "\n",
    "\n",
    "def process_braidz_files(\n",
    "    files: List[str],\n",
    "    root_folder: str,\n",
    "    output_path: str,\n",
    "    group_id: str,\n",
    "    storage_format: StorageFormat = \"parquet\",\n",
    "    compression: Optional[str] = \"snappy\",\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process braidz files and save results in efficient format for large datasets.\n",
    "\n",
    "    Args:\n",
    "        files: List of braidz filenames to process\n",
    "        root_folder: Root directory containing the braidz files\n",
    "        output_path: Directory where processed files should be saved\n",
    "        group_id: Group identifier for organizing output files\n",
    "        storage_format: Format to save data in ('parquet', 'hdf5', or 'feather')\n",
    "        compression: Compression method (format-specific, default 'snappy' for parquet)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping keys to processed DataFrames\n",
    "\n",
    "    Storage Format Details:\n",
    "        - parquet: Best for column-oriented data, good compression, splittable\n",
    "        - hdf5: Good for hierarchical data, fast random access\n",
    "        - feather: Fastest read/write times, but larger file size\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    group_output_dir = Path(output_path) / group_id\n",
    "    group_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_file_extension() -> str:\n",
    "        return {\"parquet\": \".parquet\", \"hdf5\": \".h5\", \"feather\": \".feather\"}[\n",
    "            storage_format\n",
    "        ]\n",
    "\n",
    "    def save_dataframe(df: pd.DataFrame, key: str, output_file: Path) -> None:\n",
    "        if storage_format == \"parquet\":\n",
    "            # Parquet with snappy compression (good balance of speed/size)\n",
    "            df.to_parquet(\n",
    "                output_file, compression=compression, index=False, engine=\"pyarrow\"\n",
    "            )\n",
    "        elif storage_format == \"hdf5\":\n",
    "            # HDF5 with table format for better performance with large datasets\n",
    "            df.to_hdf(\n",
    "                output_file,\n",
    "                key=key,\n",
    "                mode=\"a\",\n",
    "                format=\"table\",\n",
    "                complevel=5 if compression else 0,\n",
    "                complib=\"blosc:lz4\",\n",
    "            )\n",
    "        elif storage_format == \"feather\":\n",
    "            # Feather format with optional LZ4 compression\n",
    "            df.to_feather(output_file, compression=compression)\n",
    "\n",
    "    def read_dataframe(file_path: Path, key: str) -> pd.DataFrame:\n",
    "        if storage_format == \"parquet\":\n",
    "            return pd.read_parquet(file_path)\n",
    "        elif storage_format == \"hdf5\":\n",
    "            return pd.read_hdf(file_path, key=key)\n",
    "        elif storage_format == \"feather\":\n",
    "            return pd.read_feather(file_path)\n",
    "\n",
    "    # Check if all files already exist\n",
    "    def get_existing_data() -> Optional[Dict[str, pd.DataFrame]]:\n",
    "        try:\n",
    "            extension = get_file_extension()\n",
    "            existing_files = list(group_output_dir.glob(f\"*{extension}\"))\n",
    "\n",
    "            if not existing_files:\n",
    "                return None\n",
    "\n",
    "            if storage_format == \"hdf5\":\n",
    "                # For HDF5, all data is in one file\n",
    "                h5_file = group_output_dir / f\"data{extension}\"\n",
    "                if not h5_file.exists():\n",
    "                    return None\n",
    "                with h5py.File(h5_file, \"r\") as f:\n",
    "                    return {key: pd.read_hdf(h5_file, key=key) for key in f.keys()}\n",
    "            else:\n",
    "                # For parquet and feather, each DataFrame is in a separate file\n",
    "                return {\n",
    "                    file.stem: read_dataframe(file, file.stem)\n",
    "                    for file in existing_files\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing files: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Try to load existing data first\n",
    "    existing_data = get_existing_data()\n",
    "    if existing_data is not None:\n",
    "        print(f\"Loaded existing {storage_format} files for {group_id}\")\n",
    "        return existing_data\n",
    "\n",
    "    # Process braidz files if data doesn't exist\n",
    "    try:\n",
    "        print(f\"Processing {group_id} files: {', '.join(files)}\")\n",
    "        processed_data = fa.braidz.read_multiple_braidz(files, root_folder)\n",
    "\n",
    "        # Save processed data\n",
    "        if storage_format == \"hdf5\":\n",
    "            # Save all DataFrames to a single HDF5 file\n",
    "            output_file = group_output_dir / f\"data{get_file_extension()}\"\n",
    "            for key, df in processed_data.items():\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    continue\n",
    "                save_dataframe(df, key, output_file)\n",
    "                print(f\"Saved {key} to {output_file}\")\n",
    "        else:\n",
    "            # Save each DataFrame to a separate file\n",
    "            for key, df in processed_data.items():\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    continue\n",
    "                output_file = group_output_dir / f\"{key}{get_file_extension()}\"\n",
    "                save_dataframe(df, key, output_file)\n",
    "                print(f\"Saved {key} to {output_file}\")\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing braidz files: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Dict, Union\n",
    "import numpy as np\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "from tslearn.clustering import TimeSeriesKMeans, KShape\n",
    "\n",
    "\n",
    "def cluster_timeseries(\n",
    "    X: np.ndarray,\n",
    "    n_clusters: int = 2,\n",
    "    time_window: Optional[Tuple[int, int]] = None,\n",
    "    method: str = \"kmeans\",\n",
    "    preprocess: bool = True,\n",
    "    resample_size: Optional[int] = None,\n",
    "    metric: str = \"dtw\",\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = False,\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Cluster time series data with optional preprocessing and time window selection.\n",
    "\n",
    "    Args:\n",
    "        X: Input time series data of shape (n_samples, n_timesteps)\n",
    "        n_clusters: Number of clusters (default: 2)\n",
    "        time_window: Optional tuple of (start, end) indices for time window selection\n",
    "        method: Clustering method ('kmeans' or 'kshape', default: 'kmeans')\n",
    "        preprocess: Whether to apply mean-variance scaling (default: True)\n",
    "        resample_size: Optional size for resampling the time series\n",
    "        metric: Distance metric for kmeans (default: 'dtw', ignored for kshape)\n",
    "        random_state: Random state for reproducibility (default: 42)\n",
    "        verbose: Whether to print progress information (default: False)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cluster labels, metadata dictionary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not isinstance(X, np.ndarray) or X.ndim != 2:\n",
    "            raise ValueError(\"X must be a 2D numpy array\")\n",
    "\n",
    "        # Extract time window if specified\n",
    "        if time_window is not None:\n",
    "            if len(time_window) != 2 or time_window[0] >= time_window[1]:\n",
    "                raise ValueError(\n",
    "                    \"time_window must be a tuple of (start, end) with start < end\"\n",
    "                )\n",
    "            X = X[:, time_window[0] : time_window[1]]\n",
    "\n",
    "        # Preprocess data\n",
    "        if preprocess:\n",
    "            X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "\n",
    "        # Resample if specified\n",
    "        if resample_size is not None:\n",
    "            if resample_size <= 0:\n",
    "                raise ValueError(\"resample_size must be positive\")\n",
    "            X = TimeSeriesResampler(sz=resample_size).fit_transform(X)\n",
    "\n",
    "        # Initialize clusterer\n",
    "        if method.lower() == \"kmeans\":\n",
    "            clusterer = TimeSeriesKMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                metric=metric,\n",
    "                random_state=random_state,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "        elif method.lower() == \"kshape\":\n",
    "            clusterer = KShape(\n",
    "                n_clusters=n_clusters, random_state=random_state, verbose=verbose\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'kmeans' or 'kshape'\")\n",
    "\n",
    "        # Perform clustering\n",
    "        labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # Prepare metadata\n",
    "        metadata = {\n",
    "            \"n_samples\": len(X),\n",
    "            \"n_timesteps\": X.shape[1],\n",
    "            \"cluster_sizes\": np.bincount(labels).tolist(),\n",
    "            \"cluster_centers\": clusterer.cluster_centers_\n",
    "            if hasattr(clusterer, \"cluster_centers_\")\n",
    "            else None,\n",
    "            \"preprocessing\": \"mean_var\" if preprocess else \"none\",\n",
    "            \"method\": method,\n",
    "            \"time_window\": time_window,\n",
    "        }\n",
    "\n",
    "        return labels, metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in clustering: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_response_peak(\n",
    "    angular_velocity, start_idx, search_window=50, height=np.deg2rad(300), distance=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect if there's a saccade response within a search window after stimulus.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    angular_velocity : array-like\n",
    "        Angular velocity time series\n",
    "    start_idx : int\n",
    "        Starting index (stimulus onset)\n",
    "    search_window : int\n",
    "        Number of frames to search after stimulus\n",
    "    height : float\n",
    "        Minimum peak height to be considered a response\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int or None\n",
    "        Index of response peak if found, None otherwise\n",
    "    \"\"\"\n",
    "    from scipy.signal import find_peaks\n",
    "\n",
    "    # Define search region\n",
    "    end_idx = min(start_idx + search_window, len(angular_velocity))\n",
    "    search_region = angular_velocity[start_idx:end_idx]\n",
    "\n",
    "    # Look for peaks in both directions\n",
    "    peaks_pos, _ = find_peaks(search_region, height=height, distance=distance)\n",
    "    peaks_neg, _ = find_peaks(-search_region, height=height, distance=distance)\n",
    "\n",
    "    all_peaks = np.concatenate([peaks_pos, peaks_neg])\n",
    "    response_direction = [\"pos\"] * len(peaks_pos) + [\"neg\"] * len(peaks_neg)\n",
    "\n",
    "    sort_idx = np.argsort(all_peaks)\n",
    "    all_peaks = all_peaks[sort_idx]\n",
    "    response_direction = np.array(response_direction)[sort_idx]\n",
    "\n",
    "    if len(all_peaks) > 0:\n",
    "        # Return the first peak found (relative to stimulus onset)\n",
    "        return start_idx + all_peaks[0], response_direction[0]\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def compute_heading_change(\n",
    "    theta_unwrap,\n",
    "    center_idx,\n",
    "    k_frames=10,\n",
    "    method=\"mean\",\n",
    "    buffer_frames=None,\n",
    "    x=None,\n",
    "    y=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the change in heading around a specified index using different methods.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta_unwrap : array-like\n",
    "        Unwrapped heading angles in radians\n",
    "    center_idx : int\n",
    "        Index around which to compute change\n",
    "    k_frames : int\n",
    "        Number of frames before/after to compute change or distance from peak for vector calculation\n",
    "    method : str\n",
    "        'mean': Use mean heading with optional buffer\n",
    "        'vector': Use vectors at k distance from peak\n",
    "    buffer_frames : int or None\n",
    "        If using 'mean' method, number of frames to skip around the peak\n",
    "    x, y : array-like or None\n",
    "        Required for 'vector' method, position coordinates\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Change in heading in radians, between -π and π\n",
    "    \"\"\"\n",
    "    if method == \"mean\":\n",
    "        # Ensure bounds including buffer if specified\n",
    "        buffer = buffer_frames if buffer_frames is not None else 0\n",
    "        total_frames_needed = k_frames + buffer\n",
    "\n",
    "        if (\n",
    "            center_idx - total_frames_needed < 0\n",
    "            or center_idx + total_frames_needed >= len(theta_unwrap)\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        # Compute mean heading before and after, respecting buffer\n",
    "        pre_heading = np.mean(\n",
    "            theta_unwrap[center_idx - total_frames_needed : center_idx - buffer]\n",
    "        )\n",
    "        post_heading = np.mean(\n",
    "            theta_unwrap[center_idx + buffer : center_idx + total_frames_needed]\n",
    "        )\n",
    "\n",
    "        # Compute change and wrap to [-π, π]\n",
    "        heading_change = np.arctan2(\n",
    "            np.sin(post_heading - pre_heading), np.cos(post_heading - pre_heading)\n",
    "        )\n",
    "\n",
    "    elif method == \"vector\":\n",
    "        if x is None or y is None:\n",
    "            raise ValueError(\"x and y coordinates are required for vector method\")\n",
    "\n",
    "        # Ensure bounds\n",
    "        if center_idx - k_frames < 0 or center_idx + k_frames >= len(x):\n",
    "            return None\n",
    "\n",
    "        # Get points at distance k from peak\n",
    "        pre_x = x[center_idx - k_frames]\n",
    "        pre_y = y[center_idx - k_frames]\n",
    "        post_x = x[center_idx + k_frames]\n",
    "        post_y = y[center_idx + k_frames]\n",
    "\n",
    "        # Calculate vectors\n",
    "        pre_vector = np.array([x[center_idx] - pre_x, y[center_idx] - pre_y])\n",
    "        post_vector = np.array([post_x - x[center_idx], post_y - y[center_idx]])\n",
    "\n",
    "        # Normalize vectors\n",
    "        pre_vector = pre_vector / np.linalg.norm(pre_vector)\n",
    "        post_vector = post_vector / np.linalg.norm(post_vector)\n",
    "\n",
    "        # Calculate angle between vectors\n",
    "        dot_product = np.clip(np.dot(pre_vector, post_vector), -1.0, 1.0)\n",
    "        heading_change = np.arccos(dot_product)\n",
    "\n",
    "        # Determine sign of angle (positive for CCW, negative for CW)\n",
    "        cross_product = np.cross(pre_vector, post_vector)\n",
    "        heading_change *= np.sign(cross_product)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'mean' or 'vector'\")\n",
    "\n",
    "    return heading_change\n",
    "\n",
    "\n",
    "def process_saccade(\n",
    "    sac_idx, angular_velocity, linear_velocity, x, y, z, stim_idx, theta_unwrap\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single saccade, including heading change calculation.\n",
    "    \"\"\"\n",
    "    WINDOW_SIZE = 100\n",
    "    ARENA_BOUNDS = {\"z\": (0.05, 0.25), \"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}\n",
    "\n",
    "    # Skip if window is out of bounds\n",
    "    if sac_idx - WINDOW_SIZE < 0 or sac_idx + WINDOW_SIZE >= len(angular_velocity):\n",
    "        return None\n",
    "\n",
    "    # Skip if saccade is in stimulus window\n",
    "    if stim_idx is not None and stim_idx <= sac_idx <= stim_idx + WINDOW_SIZE:\n",
    "        return None\n",
    "\n",
    "    # Skip if outside arena bounds\n",
    "    if not (\n",
    "        ARENA_BOUNDS[\"z\"][0] <= z[sac_idx] <= ARENA_BOUNDS[\"z\"][1]\n",
    "        and ARENA_BOUNDS[\"x\"][0] <= x[sac_idx] <= ARENA_BOUNDS[\"x\"][1]\n",
    "        and ARENA_BOUNDS[\"y\"][0] <= y[sac_idx] <= ARENA_BOUNDS[\"y\"][1]\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    # Extract velocity windows\n",
    "    ang_vel_window = angular_velocity[sac_idx - WINDOW_SIZE : sac_idx + WINDOW_SIZE]\n",
    "    lin_vel_window = linear_velocity[sac_idx - WINDOW_SIZE : sac_idx + WINDOW_SIZE]\n",
    "    turning_direction = \"pos\" if angular_velocity[sac_idx] > 0 else \"neg\"\n",
    "\n",
    "    # Compute heading change\n",
    "    heading_change = compute_heading_change(\n",
    "        theta_unwrap, sac_idx, method=\"vector\", k_frames=25, x=x, y=y\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        \"spontaneous\",\n",
    "        ang_vel_window,\n",
    "        lin_vel_window,\n",
    "        heading_change,\n",
    "        turning_direction,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_stimulus_window(\n",
    "    stim_idx, angular_velocity, linear_velocity, x, y, z, theta_unwrap, default_delay=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the stimulus window, including heading change calculation.\n",
    "    \"\"\"\n",
    "    WINDOW_SIZE = 100\n",
    "    ARENA_BOUNDS = {\"z\": (0.05, 0.25), \"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}\n",
    "\n",
    "    # First check if initial window is out of bounds\n",
    "    if stim_idx - WINDOW_SIZE < 0 or stim_idx + WINDOW_SIZE >= len(angular_velocity):\n",
    "        return None\n",
    "\n",
    "    # Check arena bounds at stimulus time\n",
    "    if not (\n",
    "        ARENA_BOUNDS[\"z\"][0] <= z[stim_idx] <= ARENA_BOUNDS[\"z\"][1]\n",
    "        and ARENA_BOUNDS[\"x\"][0] <= x[stim_idx] <= ARENA_BOUNDS[\"x\"][1]\n",
    "        and ARENA_BOUNDS[\"y\"][0] <= y[stim_idx] <= ARENA_BOUNDS[\"y\"][1]\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    # Try to detect a response\n",
    "    response_idx, response_direction = detect_response_peak(\n",
    "        angular_velocity,\n",
    "        stim_idx,\n",
    "        search_window=50,\n",
    "        height=np.deg2rad(300),\n",
    "        distance=10,\n",
    "    )\n",
    "\n",
    "    if response_idx is not None:\n",
    "        # Response detected - align to response\n",
    "        center_idx = response_idx\n",
    "        group = \"stimulus_response\"\n",
    "    else:\n",
    "        # No response detected - use default delay\n",
    "        center_idx = stim_idx + default_delay\n",
    "        group = \"stimulus_no_response\"\n",
    "        response_direction = \"none\"\n",
    "\n",
    "    # Check if the aligned window is within bounds\n",
    "    if center_idx - WINDOW_SIZE < 0 or center_idx + WINDOW_SIZE >= len(\n",
    "        angular_velocity\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    # Extract velocity windows aligned to either response or default delay\n",
    "    ang_vel_window = angular_velocity[\n",
    "        center_idx - WINDOW_SIZE : center_idx + WINDOW_SIZE\n",
    "    ]\n",
    "    lin_vel_window = linear_velocity[\n",
    "        center_idx - WINDOW_SIZE : center_idx + WINDOW_SIZE\n",
    "    ]\n",
    "\n",
    "    # Compute heading change\n",
    "    heading_change = compute_heading_change(\n",
    "        theta_unwrap, center_idx, method=\"vector\", k_frames=25, x=x, y=y\n",
    "    )\n",
    "    return group, ang_vel_window, lin_vel_window, heading_change, response_direction\n",
    "\n",
    "\n",
    "def detect_saccades_both(angular_velocity, height=np.deg2rad(300), distance=40):\n",
    "    \"\"\"Detect saccades in both positive and negative directions.\"\"\"\n",
    "    from scipy.signal import find_peaks\n",
    "\n",
    "    peaks_pos, _ = find_peaks(angular_velocity, height=height, distance=distance)\n",
    "    peaks_neg, _ = find_peaks(-angular_velocity, height=height, distance=distance)\n",
    "\n",
    "    peaks_concat = np.concatenate([peaks_pos, peaks_neg])\n",
    "\n",
    "    # get directions list\n",
    "    directions = np.array([\"pos\"] * len(peaks_pos) + [\"neg\"] * len(peaks_neg))\n",
    "\n",
    "    # get sorting indices\n",
    "    sort_idx = np.argsort(peaks_concat)\n",
    "\n",
    "    # return sorted peaks and directions\n",
    "    return peaks_concat[sort_idx], np.array(directions)[sort_idx]\n",
    "\n",
    "\n",
    "def process_all_trajectories(stim, df):\n",
    "    \"\"\"\n",
    "    Process all trajectories and collect data including heading changes.\n",
    "    \"\"\"\n",
    "    angvels = []\n",
    "    linvels = []\n",
    "    groups = []\n",
    "    heading_changes = []\n",
    "    turning_directions = []\n",
    "    distance_from_center = []\n",
    "\n",
    "    for _, row in tqdm(stim.iterrows(), total=len(stim)):\n",
    "        obj_id = row[\"obj_id\"]\n",
    "        exp_num = row[\"exp_num\"]\n",
    "        frame = row[\"frame\"]\n",
    "\n",
    "        # Get trajectory data\n",
    "        grp = df[(df[\"obj_id\"] == obj_id) & (df[\"exp_num\"] == exp_num)].copy()\n",
    "\n",
    "        if len(grp) < 200:\n",
    "            continue\n",
    "\n",
    "        stim_idx = np.where(grp[\"frame\"] == frame)[0]\n",
    "        stim_idx = stim_idx[0] if len(stim_idx) > 0 else None\n",
    "\n",
    "        grp = sg_smooth(grp)\n",
    "\n",
    "        # Calculate all velocities and angles\n",
    "        x, y, z = grp[\"x\"].to_numpy(), grp[\"y\"].to_numpy(), grp[\"z\"].to_numpy()\n",
    "        xvel, yvel = grp[\"xvel\"].to_numpy(), grp[\"yvel\"].to_numpy()\n",
    "\n",
    "        theta = np.arctan2(yvel, xvel)\n",
    "        theta_unwrap = np.unwrap(theta)\n",
    "        _, angular_velocity = pynumdiff.smooth_finite_difference.butterdiff(\n",
    "            theta_unwrap, dt=0.01, params=[1, 0.1]\n",
    "        )\n",
    "        linear_velocity = np.sqrt(xvel**2 + yvel**2)\n",
    "\n",
    "        saccades, directions = detect_saccades_both(angular_velocity)\n",
    "\n",
    "        # Process spontaneous saccades\n",
    "        for sac_idx in saccades:\n",
    "            result = process_saccade(\n",
    "                sac_idx,\n",
    "                angular_velocity,\n",
    "                linear_velocity,\n",
    "                x,\n",
    "                y,\n",
    "                z,\n",
    "                stim_idx,\n",
    "                theta_unwrap,\n",
    "            )\n",
    "            if result is not None:\n",
    "                (\n",
    "                    group,\n",
    "                    ang_vel_window,\n",
    "                    lin_vel_window,\n",
    "                    heading_change,\n",
    "                    turning_direction,\n",
    "                ) = result\n",
    "                groups.append(group)\n",
    "                angvels.append(ang_vel_window)\n",
    "                linvels.append(lin_vel_window)\n",
    "                heading_changes.append(heading_change)\n",
    "                turning_directions.append(turning_direction)\n",
    "                distance_from_center.append(np.sqrt(x[sac_idx] ** 2 + y[sac_idx] ** 2))\n",
    "\n",
    "        # Process stimulus window\n",
    "        if stim_idx is not None:\n",
    "            result = process_stimulus_window(\n",
    "                stim_idx, angular_velocity, linear_velocity, x, y, z, theta_unwrap\n",
    "            )\n",
    "            if result is not None:\n",
    "                (\n",
    "                    group,\n",
    "                    ang_vel_window,\n",
    "                    lin_vel_window,\n",
    "                    heading_change,\n",
    "                    turning_direction,\n",
    "                ) = result\n",
    "                groups.append(group)\n",
    "                angvels.append(ang_vel_window)\n",
    "                linvels.append(lin_vel_window)\n",
    "                heading_changes.append(heading_change)\n",
    "                turning_directions.append(turning_direction)\n",
    "                distance_from_center.append(\n",
    "                    np.sqrt(x[stim_idx] ** 2 + y[stim_idx] ** 2)\n",
    "                )\n",
    "\n",
    "    out_dict = {\n",
    "        \"groups\": groups,\n",
    "        \"angular_velocities\": angvels,\n",
    "        \"linear_velocities\": linvels,\n",
    "        \"heading_changes\": heading_changes,\n",
    "        \"turning_directions\": turning_directions,\n",
    "        \"distance_from_center\": distance_from_center,\n",
    "    }\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_saccade_direction(velocity_windows):\n",
    "    \"\"\"\n",
    "    Align all saccades to be positive by flipping negative peaks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocity_windows : array\n",
    "        Array of velocity windows (n_saccades x window_length)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        Aligned velocity windows where all peaks are positive\n",
    "    \"\"\"\n",
    "    center_idx = velocity_windows.shape[1] // 2\n",
    "    center_values = velocity_windows[:, center_idx]\n",
    "    flip_mask = center_values < 0\n",
    "    aligned_windows = velocity_windows.copy()\n",
    "    aligned_windows[flip_mask] = -aligned_windows[flip_mask]\n",
    "    return aligned_windows\n",
    "\n",
    "\n",
    "def plot_three_panel(\n",
    "    angvels, linvels, heading_changes, groups, window_size=100, combine_stimulus=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a three-panel figure with angular velocity, linear velocity, and heading change distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    angvels : list of arrays\n",
    "        Angular velocity windows for each event\n",
    "    linvels : list of arrays\n",
    "        Linear velocity windows for each event\n",
    "    heading_changes : list of floats\n",
    "        Heading changes in radians\n",
    "    groups : list of str\n",
    "        Group labels for each event\n",
    "    window_size : int\n",
    "        Size of the window on each side (default: 100)\n",
    "    combine_stimulus : bool\n",
    "        If True, combines stimulus_response and stimulus_no_response into a single 'stimulus' group\n",
    "    \"\"\"\n",
    "    # Convert lists to arrays\n",
    "    angvels = align_saccade_direction(np.rad2deg(angvels))\n",
    "    linvels = np.array(linvels)\n",
    "    groups = np.array(groups)\n",
    "    heading_changes_deg = np.rad2deg(np.array(heading_changes))\n",
    "\n",
    "    # Combine stimulus groups if requested\n",
    "    if combine_stimulus:\n",
    "        # Create new groups array with combined stimulus\n",
    "        new_groups = np.array([\"stimulus\" if \"stimulus\" in g else g for g in groups])\n",
    "        groups = new_groups\n",
    "\n",
    "        # Update colors dictionary\n",
    "        colors = {\"spontaneous\": \"tab:blue\", \"stimulus\": \"tab:orange\"}\n",
    "    else:\n",
    "        colors = {\n",
    "            \"spontaneous\": \"tab:blue\",\n",
    "            \"stimulus_response\": \"tab:orange\",\n",
    "            \"stimulus_no_response\": \"tab:green\",\n",
    "        }\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    gs = fig.add_gridspec(1, 3, width_ratios=[1, 1, 1])\n",
    "\n",
    "    # Calculate time axis\n",
    "    time = np.linspace(-window_size, window_size, window_size * 2) * 0.01\n",
    "\n",
    "    # First panel: Angular Velocity\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    for group in np.unique(groups):\n",
    "        mask = groups == group\n",
    "        mean = np.mean(angvels[mask], axis=0)\n",
    "        std = np.std(angvels[mask], axis=0)\n",
    "        ax1.fill_between(time, mean - std, mean + std, alpha=0.3, color=colors[group])\n",
    "        ax1.plot(time, mean, color=colors[group], label=f\"{group} n={np.sum(mask)}\")\n",
    "\n",
    "    ax1.set_xlabel(\"Time (s)\")\n",
    "    ax1.set_ylabel(\"Angular Velocity (rad/s)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Second panel: Linear Velocity\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    for group in np.unique(groups):\n",
    "        mask = groups == group\n",
    "        mean = np.mean(linvels[mask], axis=0)\n",
    "        std = np.std(linvels[mask], axis=0)\n",
    "        ax2.fill_between(time, mean - std, mean + std, alpha=0.3, color=colors[group])\n",
    "        ax2.plot(time, mean, color=colors[group])\n",
    "\n",
    "    ax2.set_xlabel(\"Time (s)\")\n",
    "    ax2.set_ylabel(\"Linear Velocity (m/s)\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Third panel: Heading Change Distribution\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "\n",
    "    # Create DataFrame for seaborn\n",
    "    df = pd.DataFrame({\"Heading Change (deg)\": heading_changes_deg, \"Group\": groups})\n",
    "\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=\"Heading Change (deg)\",\n",
    "        hue=\"Group\",\n",
    "        stat=\"density\",\n",
    "        common_norm=False,\n",
    "        alpha=0.5,\n",
    "        binwidth=10,\n",
    "        palette=colors,\n",
    "        legend=False,\n",
    "        kde=True,\n",
    "        ax=ax3,\n",
    "    )\n",
    "\n",
    "    ax3.set_xlabel(\"Heading Change (degrees)\")\n",
    "    ax3.set_ylabel(\"Density\")\n",
    "    ax3.set_xlim(-180, 180)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.figlegend(loc=\"lower center\", ncol=3, bbox_to_anchor=(0.5, -0.1))\n",
    "\n",
    "    return fig, (ax1, ax2, ax3)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Plot with separate stimulus groups\n",
    "fig, axes = plot_three_panel(angvels, linvels, heading_changes, groups, combine_stimulus=False)\n",
    "\n",
    "# Plot with combined stimulus groups\n",
    "fig, axes = plot_three_panel(angvels, linvels, heading_changes, groups, combine_stimulus=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def plot_and_save_all(results, group):\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    out_folder = f\"Figures/dnp03_paper/20241105/{group}/\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    for val in [True, False]:\n",
    "        title = \"Combined\" if val else \"Separate\"\n",
    "\n",
    "        # demeaned absolute value\n",
    "        fig, axes = plot_three_panel(\n",
    "            [np.abs(av) - np.mean(np.abs(av)) for av in results[\"angular_velocities\"]],\n",
    "            results[\"linear_velocities\"],\n",
    "            results[\"heading_changes\"],\n",
    "            results[\"groups\"],\n",
    "            combine_stimulus=val,\n",
    "        )\n",
    "        fig.suptitle(f\"{group} (demeaned abs, {title})\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(out_folder, f\"{group}_abs_demean_{title}.png\")\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "\n",
    "        # absolute value\n",
    "        fig, axes = plot_three_panel(\n",
    "            np.abs(results[\"angular_velocities\"]),\n",
    "            results[\"linear_velocities\"],\n",
    "            results[\"heading_changes\"],\n",
    "            results[\"groups\"],\n",
    "            combine_stimulus=val,\n",
    "        )\n",
    "        fig.suptitle(f\"{group} (abs, {title})\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(out_folder, f\"{group}_abs_{title}.png\")\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "\n",
    "        # raw flipped\n",
    "        fig, axes = plot_three_panel(\n",
    "            results[\"angular_velocities\"],\n",
    "            results[\"linear_velocities\"],\n",
    "            results[\"heading_changes\"],\n",
    "            results[\"groups\"],\n",
    "            combine_stimulus=val,\n",
    "        )\n",
    "        fig.suptitle(f\"{group} (flipped traces, {title})\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(out_folder, f\"{group}_flipped_{title}.png\")\n",
    "        plt.savefig(save_path, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNp03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j53xu68_files = [\"20230321_162524.braidz\", \"20230519_130210.braidz\"]\n",
    "j53_data = process_braidz_files(\n",
    "    j53xu68_files, root_folder, data_path, \"dnp03\", storage_format=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j53_results = process_all_trajectories(j53_data[\"stim\"], j53_data[\"df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AX-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process g29xu68 files\n",
    "g29xu68_files = [\"20230512_144203.braidz\", \"20230203_145747.braidz\"]\n",
    "g29_data = process_braidz_files(\n",
    "    g29xu68_files, root_folder, data_path, \"ax-split\", storage_format=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_split_results = process_all_trajectories(g29_data[\"stim\"], g29_data[\"df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process emptyxu68 files\n",
    "emptyxu68_files = [\n",
    "    \"20230915_171628.braidz\",\n",
    "    \"20230206_141606.braidz\",\n",
    "]\n",
    "\n",
    "empty_data = fa.braidz.read_multiple_braidz(emptyxu68_files, root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_split_results = process_all_trajectories(empty_data[\"stim\"], empty_data[\"df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: ColorBrewer qualitative palette (colorblind-friendly)\n",
    "colors_brewer = [\"#1b9e77\", \"#d95f02\", \"#7570b3\", \"#666666\"]\n",
    "\n",
    "# Option 2: Scientific paper-inspired palette\n",
    "colors_scientific = [\"#2f5c85\", \"#ca562c\", \"#008c62\", \"#7c6f91\"]\n",
    "\n",
    "# Option 3: Science journal palette (Nature style)\n",
    "colors_nature = [\"#4c72b0\", \"#dd8452\", \"#55a868\", \"#8172b3\"]\n",
    "\n",
    "# Option 4: Paul Tol's colorblind-friendly palette\n",
    "colors_tol = [\"#332288\", \"#88CCEE\", \"#44AA99\", \"#999933\"]\n",
    "\n",
    "cp = colors_nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare angular velocities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Direction Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create long dataframe with all data\n",
    "dnp03_stimulus_mask = np.array(\n",
    "    [x.startswith(\"stimulus\") for x in j53_results[\"groups\"]]\n",
    ")\n",
    "dnp03_turning_directions = np.array(j53_results[\"turning_directions\"])[\n",
    "    dnp03_stimulus_mask\n",
    "]\n",
    "dnp03_pos_turn_count = sum(dnp03_turning_directions == \"pos\")\n",
    "dnp03_neg_turn_count = sum(dnp03_turning_directions == \"neg\")\n",
    "dnp03_none_turn_count = sum(dnp03_turning_directions == \"none\")\n",
    "\n",
    "ax_split_stimulus_mask = np.array(\n",
    "    [x.startswith(\"stimulus\") for x in ax_split_results[\"groups\"]]\n",
    ")\n",
    "ax_split_turning_directions = np.array(ax_split_results[\"turning_directions\"])[\n",
    "    ax_split_stimulus_mask\n",
    "]\n",
    "ax_split_pos_turn_count = sum(ax_split_turning_directions == \"pos\")\n",
    "ax_split_neg_turn_count = sum(ax_split_turning_directions == \"neg\")\n",
    "ax_split_none_turn_count = sum(ax_split_turning_directions == \"none\")\n",
    "\n",
    "empty_stimulus_mask = np.array(\n",
    "    [x.startswith(\"stimulus\") for x in empty_split_results[\"groups\"]]\n",
    ")\n",
    "empty_turning_directions = np.array(empty_split_results[\"turning_directions\"])[\n",
    "    empty_stimulus_mask\n",
    "]\n",
    "empty_pos_turn_count = sum(empty_turning_directions == \"pos\")\n",
    "empty_neg_turn_count = sum(empty_turning_directions == \"neg\")\n",
    "empty_none_turn_count = sum(empty_turning_directions == \"none\")\n",
    "\n",
    "\n",
    "# Create initial DataFrame with counts\n",
    "data = {\n",
    "    \"Group\": [\"DNp03\"] * 3 + [\"AX-Split\"] * 3 + [\"Empty-split\"] * 3,\n",
    "    \"Direction\": [\"Positive\", \"None\", \"Negative\"] * 3,\n",
    "    \"Count\": [\n",
    "        # DNp03 counts\n",
    "        dnp03_pos_turn_count,\n",
    "        dnp03_none_turn_count,\n",
    "        dnp03_neg_turn_count,\n",
    "        # AX-Split counts\n",
    "        ax_split_pos_turn_count,\n",
    "        ax_split_none_turn_count,\n",
    "        ax_split_neg_turn_count,\n",
    "        # Empty-split counts\n",
    "        empty_pos_turn_count,\n",
    "        empty_none_turn_count,\n",
    "        empty_neg_turn_count,\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total counts for each group\n",
    "group_totals = df.groupby(\"Group\")[\"Count\"].transform(\"sum\")\n",
    "\n",
    "# Calculate percentages\n",
    "df[\"Percentage\"] = (df[\"Count\"] / group_totals) * 100\n",
    "\n",
    "# Create the percentage plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x=\"Group\", y=\"Percentage\", hue=\"Direction\", palette=cp)\n",
    "\n",
    "plt.title(\"Turning Directions by Group (Percentage)\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for container in plt.gca().containers:\n",
    "    plt.bar_label(container, fmt=\"%.1f%%\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    f\"Figures/dnp03_paper/{date_time}/turning_directions_percentage.png\", dpi=150\n",
    ")\n",
    "plt.savefig(\n",
    "    f\"Figures/dnp03_paper/{date_time}/turning_directions_percentage.svg\", dpi=300\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Display the DataFrame with both counts and percentages\n",
    "# print(\"\\nDataFrame with counts and percentages:\")\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Direction grouped violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly_line = []\n",
    "saccade_peak_value = []\n",
    "saccade_direction = []\n",
    "\n",
    "for i, av in enumerate(j53_results[\"angular_velocities\"]):\n",
    "    if j53_results[\"groups\"][i] == \"spontaneous\":\n",
    "        continue\n",
    "\n",
    "    fly_line.append(\"DNp03\")\n",
    "    saccade_peak_value.append(av[100])\n",
    "\n",
    "    if j53_results[\"groups\"][i] == \"stimulus_response\":\n",
    "        saccade_direction.append(\n",
    "            \"pos\" if j53_results[\"turning_directions\"][i] == \"pos\" else \"neg\"\n",
    "        )\n",
    "    else:\n",
    "        saccade_direction.append(\"none\")\n",
    "\n",
    "for i, av in enumerate(ax_split_results[\"angular_velocities\"]):\n",
    "    if ax_split_results[\"groups\"][i] == \"spontaneous\":\n",
    "        continue\n",
    "\n",
    "    fly_line.append(\"AX-Split\")\n",
    "    saccade_peak_value.append(av[100])\n",
    "\n",
    "    if ax_split_results[\"groups\"][i] == \"stimulus_response\":\n",
    "        saccade_direction.append(\n",
    "            \"pos\" if ax_split_results[\"turning_directions\"][i] == \"pos\" else \"neg\"\n",
    "        )\n",
    "    else:\n",
    "        saccade_direction.append(\"none\")\n",
    "\n",
    "for i, av in enumerate(empty_split_results[\"angular_velocities\"]):\n",
    "    if empty_split_results[\"groups\"][i] == \"spontaneous\":\n",
    "        continue\n",
    "\n",
    "    fly_line.append(\"Empty-Split\")\n",
    "    saccade_peak_value.append(av[100])\n",
    "\n",
    "    if empty_split_results[\"groups\"][i] == \"stimulus_response\":\n",
    "        saccade_direction.append(\n",
    "            \"pos\" if empty_split_results[\"turning_directions\"][i] == \"pos\" else \"neg\"\n",
    "        )\n",
    "    else:\n",
    "        saccade_direction.append(\"none\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Fly Line\": fly_line,\n",
    "        \"Saccade Peak Value\": np.abs(np.rad2deg(saccade_peak_value)),\n",
    "        \"Saccade Direction\": saccade_direction,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Fly Line\",\n",
    "    y=\"Saccade Peak Value\",\n",
    "    hue=\"Saccade Direction\",\n",
    "    split=False,\n",
    "    palette=cp,\n",
    ")\n",
    "\n",
    "plt.title(\"Saccade Peak Value by Fly Line and Direction\")\n",
    "\n",
    "plt.savefig(f\"Figures/dnp03_paper/{date_time}/saccade_peak_value.png\", dpi=150)\n",
    "plt.savefig(f\"Figures/dnp03_paper/{date_time}/saccade_peak_value.svg\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_change_dnp03_stim = np.array(j53_results[\"heading_changes\"])[\n",
    "    (np.array(j53_results[\"groups\"]) == \"stimulus_response\")\n",
    "    | (np.array(j53_results[\"groups\"]) == \"stimulus_no_response\")\n",
    "]\n",
    "heading_change_ax_stim = np.array(ax_split_results[\"heading_changes\"])[\n",
    "    (np.array(ax_split_results[\"groups\"]) == \"stimulus_response\")\n",
    "    | (np.array(ax_split_results[\"groups\"]) == \"stimulus_no_response\")\n",
    "]\n",
    "heading_change_empty_stim = np.array(empty_split_results[\"heading_changes\"])[\n",
    "    (np.array(empty_split_results[\"groups\"]) == \"stimulus_response\")\n",
    "    | (np.array(empty_split_results[\"groups\"]) == \"stimulus_no_response\")\n",
    "]\n",
    "heading_change_spont_combined = np.concatenate(\n",
    "    [\n",
    "        np.array(j53_results[\"heading_changes\"])[\n",
    "            np.array(j53_results[\"groups\"]) == \"spontaneous\"\n",
    "        ],\n",
    "        np.array(ax_split_results[\"heading_changes\"])[\n",
    "            np.array(ax_split_results[\"groups\"]) == \"spontaneous\"\n",
    "        ],\n",
    "        np.array(empty_split_results[\"heading_changes\"])[\n",
    "            np.array(empty_split_results[\"groups\"]) == \"spontaneous\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "# Convert degrees to radians\n",
    "def to_rad(deg_array):\n",
    "    return np.deg2rad(deg_array)\n",
    "\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"polar\"}, figsize=(8, 8))\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "\n",
    "# Define colors for better visibility\n",
    "colors = cp\n",
    "labels = [\"DNp03\", \"AX-Split\", \"Empty-Split\", \"Spontaneous\"]\n",
    "\n",
    "datasets = [\n",
    "    heading_change_dnp03_stim,\n",
    "    heading_change_ax_stim,\n",
    "    heading_change_empty_stim,\n",
    "    heading_change_spont_combined,\n",
    "]\n",
    "\n",
    "for data, color, label in zip(datasets, colors, labels):\n",
    "    # Convert to radians\n",
    "    data_rad = data\n",
    "\n",
    "    # Create evenly spaced points for evaluation\n",
    "    theta = np.linspace(0, 2 * np.pi, 200)\n",
    "\n",
    "    # Compute KDE\n",
    "    # Duplicate some data points across boundaries to ensure continuity\n",
    "    data_wrapped = np.concatenate(\n",
    "        [data_rad, data_rad + 2 * np.pi, data_rad - 2 * np.pi]\n",
    "    )\n",
    "    kde = gaussian_kde(data_wrapped, bw_method=0.05)  # Adjust bw_method as needed\n",
    "\n",
    "    # Evaluate KDE\n",
    "    density = kde(theta)\n",
    "    # density = density / density.max()  # Normalize\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(theta, density, color=color, label=label, lw=2)\n",
    "\n",
    "# Customize plot\n",
    "# ax.set_rticks([0.2, 0.4, 0.6, 0.8, 1.0])  # Set radial ticks\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "ax.set_thetagrids(\n",
    "    np.arange(0, 360, 45),\n",
    "    labels=[\"0°\", \"45°\", \"90°\", \"135°\", \"±180°\", \"-135°\", \"-90°\", \"-45°\"],\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of heading changes for all groups\")\n",
    "plt.ylabel(\"Probability Density\", labelpad=30)\n",
    "plt.savefig(\n",
    "    f\"Figures/dnp03_paper/{date_time}/heading_change_kde_not_normalized.png\", dpi=150\n",
    ")\n",
    "plt.savefig(\n",
    "    f\"Figures/dnp03_paper/{date_time}/heading_change_kde_not_normalized.svg\", dpi=300\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnp03_angvel_stim = np.array(j53_results[\"angular_velocities\"])[\n",
    "    (np.array(j53_results[\"groups\"]) == \"stimulus_response\")\n",
    "]\n",
    "ax_angvel_stim = np.array(ax_split_results[\"angular_velocities\"])[\n",
    "    (np.array(ax_split_results[\"groups\"]) == \"stimulus_response\")\n",
    "]\n",
    "empty_angvel_stim = np.array(empty_split_results[\"angular_velocities\"])[\n",
    "    (np.array(empty_split_results[\"groups\"]) == \"stimulus_response\")\n",
    "]\n",
    "spontaneous_angvel_stim = np.concatenate(\n",
    "    [\n",
    "        np.array(j53_results[\"angular_velocities\"])[\n",
    "            np.array(j53_results[\"groups\"]) == \"spontaneous\"\n",
    "        ],\n",
    "        np.array(ax_split_results[\"angular_velocities\"])[\n",
    "            np.array(ax_split_results[\"groups\"]) == \"spontaneous\"\n",
    "        ],\n",
    "        np.array(empty_split_results[\"angular_velocities\"])[\n",
    "            np.array(empty_split_results[\"groups\"]) == \"spontaneous\"\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_mean_and_sem(arr):\n",
    "    new_arr = []\n",
    "    for a in arr:\n",
    "        a = np.rad2deg(a)\n",
    "        a = np.abs(a)\n",
    "        a = a - np.mean(a[:50])\n",
    "        new_arr.append(a)\n",
    "    arr = np.array(new_arr)\n",
    "    mean = np.mean(arr, axis=0)\n",
    "    sem = np.std(arr, axis=0)  # / np.sqrt(arr.shape[0])  # Standard Error of Mean\n",
    "    return mean, sem\n",
    "\n",
    "\n",
    "# Calculate mean and SEM\n",
    "dnp03_mean, dnp03_sem = get_mean_and_sem(dnp03_angvel_stim)\n",
    "ax_mean, ax_sem = get_mean_and_sem(ax_angvel_stim)\n",
    "empty_mean, empty_sem = get_mean_and_sem(empty_angvel_stim)\n",
    "spont_mean, spont_sem = get_mean_and_sem(spontaneous_angvel_stim)\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "time = np.linspace(-100, 100, 200) * 0.01\n",
    "\n",
    "# Plot experimental conditions in top subplot\n",
    "for mean, sem, label, color in [\n",
    "    (dnp03_mean, dnp03_sem, \"DNp03\", cp[0]),\n",
    "    (ax_mean, ax_sem, \"AX-Split\", cp[1]),\n",
    "    (empty_mean, empty_sem, \"Empty-Split\", cp[2]),\n",
    "]:\n",
    "    ax.plot(time, mean, label=label, color=color, alpha=0.8, lw=2)\n",
    "    ax.fill_between(time, mean - sem, mean + sem, alpha=0.2, color=color)\n",
    "\n",
    "# Plot spontaneous in bottom subplot\n",
    "ax.plot(time, spont_mean, label=\"Spontaneous\", color=cp[3], alpha=0.8, lw=2)\n",
    "ax.fill_between(\n",
    "    time, spont_mean - spont_sem, spont_mean + spont_sem, alpha=0.2, color=cp[3]\n",
    ")\n",
    "\n",
    "# Customize plots\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylabel(\"Angular Velocity (°/s)\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Add vertical line at t=0\n",
    "# ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_title(\"Mean Angular Velocity for Stimulus Response only (STD)\")\n",
    "\n",
    "ax.set_xlim(-0.5, 0.75)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Figures/dnp03_paper/{date_time}/angular_velocity_mean_std.png\", dpi=150)\n",
    "plt.savefig(f\"Figures/dnp03_paper/{date_time}/angular_velocity_mean_std.svg\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "\n",
    "i = 0\n",
    "obj_id = j53_data[\"stim\"][\"obj_id\"].iloc[i]\n",
    "frame = j53_data[\"stim\"][\"frame\"].iloc[i]\n",
    "exp_num = j53_data[\"stim\"][\"exp_num\"].iloc[i]\n",
    "\n",
    "grp = j53_data[\"df\"][\n",
    "    (j53_data[\"df\"][\"obj_id\"] == obj_id) & (j53_data[\"df\"][\"exp_num\"] == exp_num)\n",
    "].copy()\n",
    "\n",
    "stim_idx = np.where(grp[\"frame\"] == frame)[0][0]\n",
    "\n",
    "# Smooth data\n",
    "grp = sg_smooth(grp)\n",
    "\n",
    "# Calculate angular and linear velocity\n",
    "xvel, yvel = grp[\"xvel\"].to_numpy(), grp[\"yvel\"].to_numpy()\n",
    "x, y, z = grp[\"x\"].to_numpy(), grp[\"y\"].to_numpy(), grp[\"z\"].to_numpy()\n",
    "\n",
    "# Calculate angular and linear velocity\n",
    "theta = np.arctan2(yvel, xvel)\n",
    "theta_unwrap = np.unwrap(theta)\n",
    "_, angular_velocity = pynumdiff.smooth_finite_difference.butterdiff(\n",
    "    theta_unwrap, dt=0.01, params=[1, 0.1]\n",
    ")\n",
    "linear_velocity = np.sqrt(xvel**2 + yvel**2)\n",
    "\n",
    "saccades, _ = scipy.signal.find_peaks(\n",
    "    np.abs(angular_velocity),\n",
    "    height=np.deg2rad(300),\n",
    "    distance=10,\n",
    "    prominence=5,\n",
    ")\n",
    "\n",
    "# find saccade immediatly after stim_idx\n",
    "stim_saccade_idx = np.where((saccades > stim_idx) & (saccades < stim_idx + 30))[0][0]\n",
    "stim_saccade = saccades[stim_saccade_idx]\n",
    "saccades = np.delete(saccades, stim_saccade_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_velocity_inset(\n",
    "    parent_ax,\n",
    "    x_pos,\n",
    "    time_data,\n",
    "    velocity_data,\n",
    "    saccade_idx,\n",
    "    color,\n",
    "    y_limits,\n",
    "    window=30,\n",
    "    connect_pos=(1, 2),\n",
    "):\n",
    "    \"\"\"Create an inset with consistent formatting\"\"\"\n",
    "    inset = parent_ax.inset_axes([x_pos, 0.7, 0.15, 0.5])  # Made insets larger\n",
    "    inset.plot(time_data, velocity_data, color=\"black\", lw=1)\n",
    "    inset.scatter(\n",
    "        time_data[saccade_idx],\n",
    "        velocity_data[saccade_idx],\n",
    "        s=sz/2,\n",
    "        marker=\"o\",\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "    left_idx = max(0, saccade_idx - window)\n",
    "    right_idx = min(len(time_data), saccade_idx + window)\n",
    "    inset.set_xlim(time_data[left_idx], time_data[right_idx])\n",
    "    inset.set_ylim(y_limits)\n",
    "\n",
    "    # Add light gray background\n",
    "    inset.set_facecolor(\"whitesmoke\")\n",
    "\n",
    "    mark_inset(\n",
    "        parent_ax,\n",
    "        inset,\n",
    "        loc1=connect_pos[0],\n",
    "        loc2=connect_pos[1],\n",
    "        fc=\"none\",\n",
    "        ec=\"0.5\",\n",
    "        lw=0.75,\n",
    "    )\n",
    "    inset.set_xticks([])\n",
    "    inset.set_yticks([])\n",
    "\n",
    "    inset.spines[\"top\"].set_visible(True)\n",
    "    inset.spines[\"right\"].set_visible(True)\n",
    "    inset.spines[\"bottom\"].set_visible(True)\n",
    "    inset.spines[\"left\"].set_visible(True)\n",
    "    inset.spines[\"top\"].set_color(\"gray\")\n",
    "    inset.spines[\"right\"].set_color(\"gray\")\n",
    "    inset.spines[\"bottom\"].set_color(\"gray\")\n",
    "    inset.spines[\"left\"].set_color(\"gray\")\n",
    "\n",
    "    return inset\n",
    "\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Set global font size\n",
    "plt.rcParams.update({\n",
    "    'font.size': 8,\n",
    "    'axes.labelsize': 8,\n",
    "    'axes.titlesize': 8,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 8,\n",
    "    'legend.fontsize': 8\n",
    "})\n",
    "\n",
    "# A4 width is 8.27 inches, let's use 7.5 inches to allow for margins\n",
    "fig_width = 8.27  # inches\n",
    "fig_height = fig_width/2  # maintain your 2:1 aspect ratio\n",
    "\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "sz = 15\n",
    "X_time = np.linspace(0, len(x) * 0.01, len(x))\n",
    "\n",
    "## Plot 1: Trajectory\n",
    "ax1.plot(x, y, color=\"black\", lw=1)\n",
    "ax1.scatter(x[saccades], y[saccades], s=sz, marker=\"o\", color=\"red\")\n",
    "ax1.scatter(x[stim_saccade], y[stim_saccade], s=sz * 1.5, marker=\"o\", color=\"blue\")\n",
    "\n",
    "# Improved arena visualization\n",
    "opto_circle = plt.Circle((0, 0), 0.025, color=\"gray\", alpha=0.2)\n",
    "ax1.add_artist(opto_circle)\n",
    "\n",
    "# Get the current axis limits\n",
    "xlim = ax1.get_xlim()\n",
    "ylim = ax1.get_ylim()\n",
    "\n",
    "# Add scale bar - using data coordinates and showing 5cm\n",
    "start_x = xlim[1] - 0.15  # Place it 0.15m from the right edge\n",
    "start_y = ylim[0] + 0.05  # Place it 0.05m from the bottom\n",
    "ax1.plot([start_x, start_x + 0.05], [start_y, start_y], 'k-', lw=2)  # 5cm = 0.05m\n",
    "ax1.text(start_x + 0.025, start_y - 0.02, '5 cm', ha='center', va='top', fontsize=8)\n",
    "\n",
    "ax1.spines[\"bottom\"].set_visible(False)\n",
    "ax1.spines[\"left\"].set_visible(False)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_aspect(\"equal\")\n",
    "\n",
    "## Plot 2: Linear Velocity\n",
    "ax2.plot(X_time, linear_velocity, color=\"black\", lw=0.8)\n",
    "ax2.scatter(X_time[saccades], linear_velocity[saccades], s=sz, marker=\"o\", color=\"red\")\n",
    "ax2.scatter(\n",
    "    X_time[stim_saccade],\n",
    "    linear_velocity[stim_saccade],\n",
    "    s=sz * 1.5,\n",
    "    marker=\"o\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax2.axvspan(X_time[stim_idx], X_time[stim_idx] + 0.3, color=\"gray\", alpha=0.2)\n",
    "\n",
    "# Create insets with same y-limits for better comparison\n",
    "linear_vel_ylim = (0.0, 0.4)\n",
    "random_saccade = saccades[6]\n",
    "create_velocity_inset(\n",
    "    ax2,\n",
    "    0.05,\n",
    "    X_time,\n",
    "    linear_velocity,\n",
    "    random_saccade,\n",
    "    \"red\",\n",
    "    linear_vel_ylim,\n",
    "    connect_pos=(1, 3),\n",
    ")\n",
    "create_velocity_inset(\n",
    "    ax2,\n",
    "    0.9,\n",
    "    X_time,\n",
    "    linear_velocity,\n",
    "    stim_saccade,\n",
    "    \"blue\",\n",
    "    linear_vel_ylim,\n",
    "    connect_pos=(2, 4),\n",
    ")\n",
    "\n",
    "## Plot 3: Angular velocity\n",
    "ax3.plot(X_time, angular_velocity, color=\"black\", lw=0.8)\n",
    "ax3.scatter(X_time[saccades], angular_velocity[saccades], s=sz, marker=\"o\", color=\"red\")\n",
    "ax3.scatter(\n",
    "    X_time[stim_saccade],\n",
    "    angular_velocity[stim_saccade],\n",
    "    s=sz * 1.5,\n",
    "    marker=\"o\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax3.axvspan(X_time[stim_idx], X_time[stim_idx] + 0.3, color=\"gray\", alpha=0.2)\n",
    "\n",
    "# Create insets with same y-limits for better comparison\n",
    "angular_vel_ylim = (np.deg2rad(-2000), np.deg2rad(2500))\n",
    "create_velocity_inset(\n",
    "    ax3,\n",
    "    0.05,\n",
    "    X_time,\n",
    "    angular_velocity,\n",
    "    random_saccade,\n",
    "    \"red\",\n",
    "    angular_vel_ylim,\n",
    "    connect_pos=(1, 3),\n",
    ")\n",
    "create_velocity_inset(\n",
    "    ax3,\n",
    "    0.9,\n",
    "    X_time,\n",
    "    angular_velocity,\n",
    "    stim_saccade,\n",
    "    \"blue\",\n",
    "    angular_vel_ylim,\n",
    "    connect_pos=(2, 4),\n",
    ")\n",
    "\n",
    "# Formatting improvements\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Y-axis formatting for angular velocity\n",
    "yticks = np.arange(-2000, 2001, 1000)\n",
    "ax3.set_yticks(np.deg2rad(yticks))\n",
    "ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f\"{np.rad2deg(x):.0f}\"))\n",
    "\n",
    "# Labels and limits\n",
    "#ax2.set_xlabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Linear Velocity (m/s)\")\n",
    "ax3.set_xlabel(\"Time (s)\")\n",
    "ax3.set_ylabel(\"Angular Velocity (deg/s)\")\n",
    "\n",
    "ax2.set_ylim(0.0, 0.4)\n",
    "ax3.set_ylim(np.deg2rad(-2000), np.deg2rad(2500))\n",
    "ax2.set_xlim(0, max(X_time))\n",
    "ax3.set_xlim(0, max(X_time))\n",
    "\n",
    "#ax3.tick_params\n",
    "ax2.set_xticklabels([])\n",
    "\n",
    "ax3.axhline(y = np.deg2rad(300), color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.axhline(y = -np.deg2rad(300), color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add annotations\n",
    "fig.text(0.02, 0.96, 'A', fontsize=16, fontweight='bold', va='top')\n",
    "fig.text(0.45, 0.96, 'B', fontsize=16, fontweight='bold', va='top')\n",
    "fig.text(0.45, 0.96/2, 'C', fontsize=16, fontweight='bold', va='top')\n",
    "\n",
    "# If you need to adjust the layout to make room for the labels\n",
    "plt.tight_layout(rect=[0.03, 0, 1, 1])  # Left margin slightly larger to accommodate labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.rcParams['font.size'] = 8.\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
    "\n",
    "# Optional: If you want to ensure text is exported properly in PDFs\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "fig.align_ylabels([ax2, ax3])\n",
    "# Save the figure\n",
    "plt.savefig(f'Figures/dnp03_paper/{date_time}/saccade_example.png', \n",
    "            dpi=300,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            format='png')\n",
    "\n",
    "# Or save as PDF (better for publications)\n",
    "plt.savefig(f'Figures/dnp03_paper/{date_time}/saccade_example.pdf',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flyanalysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
